{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "**PROJECTO 1** <br>\n",
    "**Análisis exploratorio y modelado predictivo de precios de viviendas en Barcelona usando Python y SQL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "Desarrollar un análisis completo y un modelo predictivo para los precios de viviendas en Barcelona, utilizando datos extraídos del portal Fotocasa. El objetivo es aplicar técnicas de extracción, manipulación y análisis de datos, así como algoritmos de Machine Learning, para predecir los precios de las viviendas en función de diversas características."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "- **price**: The price of the real-state.\n",
    "- **rooms**: Number of rooms.\n",
    "- **bathroom**: Number of bathrooms.\n",
    "- **lift**: whether a building has an elevator (also known as a lift in some regions) or not\n",
    "- **terrace**: If it has a terrace or not.\n",
    "- **square_meters**: Number of square meters.\n",
    "- **real_state**: Kind of real-state.\n",
    "- **neighborhood**: Neighborhood\n",
    "- **square_meters_price**: Price of the square meter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# To help with data visualization\n",
    "import matplotlib.pyplot as plt # data visualization\n",
    "import seaborn as sns # data visualization\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid') # set style for visualization\n",
    "\n",
    "# To supress warnings\n",
    "import warnings # ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#normalizing\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler # to scale the data\n",
    "\n",
    "# modeling\n",
    "import statsmodels.api as sm # adding a constant to the independent variables\n",
    "from sklearn.model_selection import train_test_split # splitting data in train and test sets\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor #To check multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Barcelona_Fotocasa_HousingPrices.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() # preview a sample first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail() # preview a sample last 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(20) # preview a sample random n rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are\", df.shape[0], 'rows and', df.shape[1], \"columns.\") # number of observations and features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes # data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=\"all\").T # statistical summary of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniques\n",
    "df.nunique() # Checking for number of variations in the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns: # Checking uniques\n",
    "    print (i,\": \",df[i].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniques\n",
    "cat_cols = df.select_dtypes(include=['category', 'object','bool']).columns.tolist()\n",
    "for column in cat_cols:\n",
    "    print(df[column].value_counts())\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicates\n",
    "df.duplicated().sum() # Checking for duplicate entries in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are 8188 rows and 10 columns.\n",
    "- The variable 'Unnamed' represent index and should be deleted from data\n",
    "- Data types are aligned with information\n",
    "- There is missing data (NaN) on variable 'real_state'. To be replaced by \"unknown\"\n",
    "- There are four types of real states being the most common \"flat\"\n",
    "- Most units do not have terrace\n",
    "- Most units do have lift\n",
    "- The neighborhood with largest unit count is \"Eixample\"\n",
    "- Units size goes from 10m2 to 679m2, with a mean of 84.61m2\n",
    "- Units prices goes from 320EUR to 15000EUR/month, with mean of 1444EUR/month\n",
    "- price range is assumed referred to monthly rent, so considered as EUR per month\n",
    "- Units prices by square meter goes from 4.9EUR/m2/month to 186EUR/m2/month, with mean of 17.7EUR/m2/month\n",
    "- There are units listed with cero rooms\n",
    "- Target variable for modeling is \"priceS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_numerical(data):\n",
    "    '''\n",
    "    Function to generate two plots for each numerical variable\n",
    "    Histplot for variable distribution\n",
    "    Boxplot for statistical summary \n",
    "    '''\n",
    "    # Select numerical columns\n",
    "    numerical_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    # Determine the number of rows and columns\n",
    "    num_vars = len(numerical_cols)\n",
    "    num_cols = 4\n",
    "    num_rows = int(np.ceil(num_vars * 2 / num_cols))\n",
    "    \n",
    "    # Create a figure with the specified size\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(5*num_cols, num_rows * 5))\n",
    "    \n",
    "    # Flatten the axes array for easy iteration\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Plot each variable with a histplot and a boxplot\n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        mean_value = data[col].mean()\n",
    "        \n",
    "        # Histplot with KDE\n",
    "        sns.histplot(data[col], kde=True, ax=axes[i*2])\n",
    "        axes[i*2].axvline(mean_value, color='r', linestyle='--')\n",
    "        axes[i*2].set_title(f'Distribution of {col}')\n",
    "        axes[i*2].text(mean_value, axes[i*2].get_ylim()[1]*0.8, f'Mean: {mean_value:.2f}', color='r', va='baseline', ha='left',rotation=90)\n",
    "        \n",
    "        # Boxplot\n",
    "        sns.boxplot(y=data[col], ax=axes[i*2 + 1])\n",
    "        axes[i*2 + 1].axhline(mean_value, color='r', linestyle='--')\n",
    "        axes[i*2 + 1].set_title(f'Boxplot of {col}')\n",
    "        axes[i*2 + 1].text(axes[i*2 + 1].get_xlim()[1]*0.8, mean_value, f'mean: {mean_value:.2f}', color='r', va='baseline', ha='right')\n",
    "    \n",
    "    # Hide any remaining empty subplots\n",
    "    for j in range(num_vars * 2, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_categorical(data):\n",
    "    '''\n",
    "    Function to generate countplot for each categorical variable\n",
    "    Labeled with count and percentage\n",
    "    '''\n",
    "    # List of categorical columns\n",
    "    categorical_columns = data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    # Number of columns in the grid\n",
    "    num_cols = 4\n",
    "    \n",
    "    # Calculate the number of rows needed\n",
    "    num_rows = (len(categorical_columns) + num_cols - 1) // num_cols\n",
    "    \n",
    "    # Create the grid\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(5*num_cols, num_rows * 5), constrained_layout=True)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Plot each countplot in the grid\n",
    "    for i, col in enumerate(categorical_columns):\n",
    "        ax = axes[i]\n",
    "        plot = sns.countplot(x=col, data=data, order=data[col].value_counts().index, ax=ax)\n",
    "        ax.set_title(f'Count of {col}')\n",
    "           \n",
    "        # Add total count and percentage annotations\n",
    "        total = len(data)\n",
    "        for p in plot.patches:\n",
    "            height = p.get_height()\n",
    "            percentage = f'{(height / total * 100):.1f}%'\n",
    "            plot.text(x=p.get_x() + p.get_width() / 2,\n",
    "                      y=height + 2,\n",
    "                      s=f'{height:.0f}\\n({percentage})',\n",
    "                      ha='center')\n",
    "        \n",
    "        # Limit x-axis labels to avoid overlap\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Remove any empty subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot crosstab with labels\n",
    "def plot_crosstab_bar_count(df, var_interest):\n",
    "    '''\n",
    "    Function to create a barplot of crosstab of the variable of interest vs each of the rest of categorical variables\n",
    "    Labeled with counts\n",
    "    '''\n",
    "    # Extract categorical columns excluding the variable of interest\n",
    "    cat_cols = df.select_dtypes(include=['category', 'object','bool']).columns.tolist()\n",
    "    cat_cols.remove(var_interest)\n",
    "    \n",
    "    # Determine the grid size\n",
    "    num_vars = len(cat_cols)\n",
    "    num_cols = 3  # Number of columns in the grid\n",
    "    num_rows = (num_vars // num_cols) + int(num_vars % num_cols > 0)\n",
    "\n",
    "    # Create a grid of subplots\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(5*num_cols, num_rows * 5), constrained_layout=True)\n",
    "    axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
    "\n",
    "    for i, col in enumerate(cat_cols):\n",
    "        # Create a crosstab\n",
    "        crosstab = pd.crosstab(df[col], df[var_interest])\n",
    "        \n",
    "        # Plot the crosstab as a bar plot\n",
    "        crosstab.plot(kind='bar', stacked=True, ax=axes[i])\n",
    "        \n",
    "        # Annotate counts in the middle of each bar section\n",
    "        for bar in axes[i].patches:\n",
    "            height = bar.get_height()\n",
    "            if height > 0:\n",
    "                axes[i].annotate(f'{int(height)}', \n",
    "                                 (bar.get_x() + bar.get_width() / 2, bar.get_y() + height / 2),\n",
    "                                 ha='center', va='center', fontsize=10, color='black')\n",
    "        \n",
    "        # Add total labels at the top of each bar\n",
    "        totals = crosstab.sum(axis=1)\n",
    "        for j, total in enumerate(totals):\n",
    "            axes[i].annotate(f'Total: {total}', \n",
    "                             (j, totals[j]), \n",
    "                             ha='center', va='bottom', weight='bold')\n",
    "\n",
    "    # Hide any remaining empty subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "#plot_crosstab_bar_count(df, var_interest='var_interest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_crosstab_heat_perc(df, var_interest,df_name=\"DataFrame\"):\n",
    "    '''\n",
    "    Function to create a heatmap of crosstab of the variable of interest vs each of the rest of catagorical variables\n",
    "    Labeled with counts, percentage by row, percentage by column\n",
    "    '''\n",
    "    # Extract categorical columns excluding the variable of interest\n",
    "    cat_cols = df.select_dtypes(include=['category', 'object']).columns.tolist()\n",
    "    cat_cols.remove(var_interest)\n",
    "    \n",
    "    # Determine the grid size\n",
    "    num_vars = len(cat_cols)\n",
    "    num_cols = 3  # Number of columns in the grid\n",
    "    num_rows = (num_vars // num_cols) + int(num_vars % num_cols > 0)\n",
    "\n",
    "    # Create a grid of subplots\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(6*num_cols, num_rows * 6))\n",
    "    axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
    "\n",
    "    for i, col in enumerate(cat_cols):\n",
    "        # Create crosstabs\n",
    "        crosstab = pd.crosstab(df[col], df[var_interest])\n",
    "        crosstab_perc_row = crosstab.div(crosstab.sum(axis=1), axis=0) * 100\n",
    "        crosstab_perc_col = crosstab.div(crosstab.sum(axis=0), axis=1) * 100\n",
    "\n",
    "        # Combine counts with percentages\n",
    "        crosstab_combined = crosstab.astype(str) + \"\\n\" + \\\n",
    "                            crosstab_perc_row.round(2).astype(str) + \"%\" + \"\\n\" + \\\n",
    "                            crosstab_perc_col.round(2).astype(str) + \"%\"\n",
    "\n",
    "        # Plot the crosstab as a heatmap\n",
    "        sns.heatmap(crosstab, annot=crosstab_combined, fmt='', cmap='Blues', ax=axes[i], cbar=False, annot_kws={\"size\": 8})\n",
    "        axes[i].set_title(f'Crosstab of {col} and {var_interest} - {df_name}', fontsize=12)\n",
    "\n",
    "    # Hide any remaining empty subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    # Adjust layout to prevent label overlapping\n",
    "    plt.subplots_adjust(hspace=0.4, wspace=0.4)  # Add more space between subplots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# Usage\n",
    "#plot_crosstab_heat_perc(df, var_interest='var_interest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplot_by_group(df, group, var, outliers, df_name=\"DataFrame\"):\n",
    "    '''\n",
    "    boxplot for a numerical variable of interest vs a categorical variable\n",
    "    with or without outliers\n",
    "    includes data mean and mean by category\n",
    "    '''\n",
    "    # Calculate the average for the variable\n",
    "    var_avg = df[var].mean()\n",
    "    \n",
    "    # Calculate variable mean per group\n",
    "    var_means = df.groupby(group)[var].mean()\n",
    "    \n",
    "    # Sort by means and get the sorted order\n",
    "    var_sorted = var_means.sort_values(ascending=False).index\n",
    "    \n",
    "    # Reorder the DataFrame by the sorted group\n",
    "    df[group] = pd.Categorical(df[group], categories=var_sorted, ordered=True)\n",
    "    \n",
    "    # Create the boxplot with the reordered sectors\n",
    "    ax = sns.boxplot(data=df, x=group, y=var, order=var_sorted, showfliers=outliers)\n",
    "    \n",
    "    # Add horizontal line for average variable value\n",
    "    plt.axhline(var_avg, color='red', linestyle='--', label=f'Avg {var}: {var_avg:.2f}')\n",
    "    \n",
    "    # Scatter plot for means\n",
    "    x_positions = range(len(var_means.sort_values(ascending=False)))\n",
    "    plt.scatter(x=x_positions, y=var_means.sort_values(ascending=False), color='red', label='Mean', zorder=5)\n",
    "    \n",
    "    # Add labels to each red dot with the mean value\n",
    "    for i, mean in enumerate(var_means.sort_values(ascending=False)):\n",
    "        plt.text(i, mean, f'{mean:.2f}', color='red', ha='center', va='bottom')\n",
    "    \n",
    "    # Rotate x-axis labels\n",
    "    plt.xticks(ticks=x_positions, labels=var_means.sort_values(ascending=False).index, rotation=90)\n",
    "    \n",
    "    # Add a legend\n",
    "    plt.legend()\n",
    "    plt.xlabel('')  # Remove x-axis title\n",
    "    \n",
    "    # Add plot title with DataFrame name\n",
    "    plt.title(f'Boxplot of {var} by {group} - {df_name}')\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Display the plot\n",
    "    #plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions:**\n",
    "- univariate_numerical(data): Function to generate two plots for each numerical variable. Histplot for variable distribution. Boxplot for statistical summary\n",
    "- univariate_categorical(data): Function to generate countplot for each categorical variable. Labeled with count and percentage\n",
    "- plot_crosstab_bar_count(df, var_interest): Function to create a barplot of crosstab of the variable of interest vs each of the rest of categorical variables. Labeled with counts\n",
    "- plot_crosstab_heat_perc(df, var_interest): Function to create a heatmap of crosstab of the variable of interest vs each of the rest of catagorical variables. Labeled with counts, percentage by row, percentage by column\n",
    "- boxplot_by_group(df, group, var, outliers): boxplot for a numerical variable of interest vs a categorical variable. with or without outliers. includes data mean and mean by category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "univariate_numerical(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "univariate_categorical(df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['real_state']==\"flat\")].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['neighborhood']==\"Eixample\")].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PairGrid\n",
    "g = sns.PairGrid(df, corner=True)\n",
    "\n",
    "# Map different plots to the grid\n",
    "g.map_lower(sns.scatterplot)\n",
    "g.map_diag(sns.histplot,kde=True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "corr_matrix = df.select_dtypes(include=np.number).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation matrix as heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the sorted correlation table\n",
    "corr_unstacked = corr_matrix.unstack() # Unstack the correlation matrix\n",
    "corr_unstacked = corr_unstacked.reset_index() # Reset the index to get 'variable1' and 'variable2' as columns\n",
    "corr_unstacked.columns = ['variable1', 'variable2', 'correlation']# Rename the columns for better understanding\n",
    "corr_unstacked = corr_unstacked[corr_unstacked['variable1'] != corr_unstacked['variable2']] # Remove self-correlations by filtering out rows where variable1 == variable2\n",
    "corr_unstacked = corr_unstacked.drop_duplicates(subset=['correlation']) # Drop duplicates to keep only one entry per variable pair\n",
    "sorted_corr = corr_unstacked.sort_values(by='correlation', key=abs, ascending=False) # Sort the DataFrame by the absolute value of correlation\n",
    "#sorted_corr # Display the sorted correlation table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to categorize the correlation level\n",
    "def categorize_correlation(correlation):\n",
    "    abs_corr = abs(correlation) * 100  # Convert to percentage for easier comparison\n",
    "    if abs_corr < 30:\n",
    "        return 'Negligible'\n",
    "    elif 30 <= abs_corr < 50:\n",
    "        return 'Low'\n",
    "    elif 50 <= abs_corr < 70:\n",
    "        return 'Moderate'\n",
    "    elif 70 <= abs_corr < 90:\n",
    "        return 'High'\n",
    "    else:\n",
    "        return 'Very High'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to create the corr_lvl column\n",
    "sorted_corr['corr_lvl'] = sorted_corr['correlation'].apply(categorize_correlation)\n",
    "sorted_corr['corr_lvl'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unique rooms-bathroom combinations\n",
    "unique_combinations=df.groupby(['rooms', 'bathroom']).size().reset_index(name='count')\n",
    "unique_combinations_sorted=unique_combinations.sort_values(by='count',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative sum of counts\n",
    "unique_combinations_sorted['cum_sum'] = unique_combinations_sorted['count'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cumulative percentage\n",
    "unique_combinations_sorted['perc'] = unique_combinations_sorted['count'] / unique_combinations_sorted['count'].sum() * 100\n",
    "unique_combinations_sorted['cum_perc'] = unique_combinations_sorted['cum_sum'] / unique_combinations_sorted['count'].sum() * 100\n",
    "unique_combinations_sorted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pop = df.merge(unique_combinations_sorted.head(10), on=['rooms', 'bathroom'])\n",
    "df_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape,df_pop.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to create and display side-by-side boxplots\n",
    "def side_by_side_boxplot(df1, df2, group, var, outliers, title1, title2):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 6), sharey=True)\n",
    "    \n",
    "    # First subplot for df1\n",
    "    plt.sca(axes[0])\n",
    "    boxplot_by_group(df1, group, var, outliers, title1)\n",
    "    \n",
    "    # Second subplot for df2\n",
    "    plt.sca(axes[1])\n",
    "    boxplot_by_group(df2, group, var, outliers, title2)\n",
    "    \n",
    "    # Show both plots after setup\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "#side_by_side_boxplot(df, df_pop, 'neighborhood', 'price', True, \"All units (show outliers)\", \"Popular units (show outliers)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_by_side_boxplot(df, df_pop, 'neighborhood', 'price', True, \"All units (show outliers)\", \"Popular units (show outliers)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_by_side_boxplot(df, df_pop, 'neighborhood', 'price', False, \"All units (without outliers)\", \"Popular units (without outliers)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_by_side_boxplot(df, df_pop, 'neighborhood', 'square_meters_price', False, \"All units (without outliers)\", \"Popular units (without outliers)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_by_side_boxplot(df, df_pop, 'neighborhood', 'square_meters', False, \"All units (without outliers)\", \"Popular units (without outliers)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_by_side_boxplot(df, df_pop, 'real_state' , 'price', False, \"All units (without outliers)\", \"Popular units (without outliers)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_by_side_boxplot(df, df_pop, 'real_state' , 'square_meters_price', False, \"All units (without outliers)\", \"Popular units (without outliers)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_by_side_boxplot(df, df_pop, 'real_state' , 'square_meters', False, \"All units (without outliers)\", \"Popular units (without outliers)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_crosstab_heat_perc(df, var_interest='real_state',df_name=\"All units\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_crosstab_heat_perc(df_pop, var_interest='real_state',df_name=\"Popular units\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_crosstab_bar_count(data, var_interest='lift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_crosstab_bar_count(data, var_interest='terrace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Functions:**\n",
    "- univariate_numerical(data): Function to generate two plots for each numerical variable. Histplot for variable distribution. Boxplot for statistical summary\n",
    "- univariate_categorical(data): Function to generate countplot for each categorical variable. Labeled with count and percentage\n",
    "- plot_crosstab_bar_count(df, var_interest): Function to create a barplot of crosstab of the variable of interest vs each of the rest of categorical variables. Labeled with counts\n",
    "- plot_crosstab_heat_perc(df, var_interest): Function to create a heatmap of crosstab of the variable of interest vs each of the rest of catagorical variables. Labeled with counts, percentage by row, percentage by column\n",
    "- boxplot_by_group(df, group, var, outliers): boxplot for a numerical variable of interest vs a categorical variable. with or without outliers. includes data mean and mean by category\n",
    "\n",
    "**Univariate Analysis**\n",
    "- The variables \"Unnamed: 0\" have a uniform distribution\n",
    "- The numerical variables have a shift to the right\n",
    "- The categorical variables are not balanced, with 79% of properties as \"apartments\" and 78% of units concentrated in 50% of the sample neighbourhoods\n",
    "- 75% of the apartment units have up to 3 bedrooms and up to 2 bathrooms with an average size of 85m2.\n",
    "- 75% of the units in Eixample have up to 3 bedrooms and up to 2 bathrooms with an average size of 87m2.\n",
    "\n",
    "\n",
    "**Bivariate Analysis**\n",
    "- 'square_meters' has a positive correlation with 'price', 'rooms' and 'bathrooms'\n",
    "- 'square_meters_price' has a negative correlation with 'square_meters', 'rooms' and 'bathrooms'\n",
    "- There are only one couple of variables with high correlation: bathroom-square_meters (0.75)\n",
    "- The most popular unit configuration in the dataset is 2 bedrooms and 1 bathroom with 1836 units (21% of all units).\n",
    "- Other popular configurations are 1-1 (18%), 3-2 (14%), 3-1 (13%), 2-2 (9%) and 4-2 (8%)\n",
    "- These six most popular unit configurations represent 86% of all units\n",
    "- The \"df_pop\" data frame includes the most popular units in terms of bedroom/bathroom configuration, representing 94% of the samples\n",
    "- Sarrià-Sant Gervasi, Les Corts, Eixample and San Martí are the most expensive neighbourhoods with average prices above the dataset average.\n",
    "- Sants-Montjuïc, Horta-Guinardó, Sant Andreu and Nou Barris are the cheapest neighbourhoods with average prices below the dataset average.\n",
    "- When comparing the price per square metre, Ciutat Vella and Eixample are the most expensive neighbourhoods. - If we compare square meters, Ciutat Vella is the second lowest and Eixample the third\n",
    "- From the perspective of price per square meter, the most attractive neighborhood according to this data could be Les Corts, with an average surface area of ​​89.79 m2 above the average (78.67 m2) and a price per square meter of 15.85 below the average (17.79)\n",
    "- From the perspective of price per square meter, the most attractive type of unit according to this data could be the apartment, with an average surface area of ​​80 m2 above the average (78.67 m2) and a price per square meter of 15.76 below the average (17.79)\n",
    "- There are 1,777 flats in Eixample, being the most popular unit type and neighborhood combination, with 79.68% of the units in Eixample being flats, and 28.9% of the flats are in Eixample. - In Les Courts there are only 398 flats, which makes it far from the most popular type of housing and neighbourhood combination, although 87.67% of the dwellings in Les Courts are flats, only 6.47% of the flats are in Les Courts.\n",
    "- Most types of units have a lift, in the case of flats the proportion is 74.12%\n",
    "- Units with a terrace on the other hand, seem to be rare and very few have one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocesing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Missing value treatment\n",
    "- Feature engineering\n",
    "- Outlier detection and treatment\n",
    "- Any other preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.copy() # Data preprocesing over a copy of original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.isna().sum() # missing values per feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['real_state'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'unknown' to categories\n",
    "df2['real_state'] = df2['real_state'].cat.add_categories(\"unknown\")\n",
    "\n",
    "# Replace NaN values with 'unknown'\n",
    "df2['real_state'] = df2['real_state'].fillna(\"unknown\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.isna().sum() # missing values per feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['real_state'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop(['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to check for outliers\n",
    "def count_outliers(df):\n",
    "    outlier_count=0\n",
    "    for column in df.select_dtypes(include=np.number).columns:\n",
    "        outliers=len(df[(df[column] < df[column].quantile(0.25)-1.5*(df[column].quantile(0.75)-df[column].quantile(0.25))) | (df[column] > df[column].quantile(0.75)+1.5*(df[column].quantile(0.75)-df[column].quantile(0.25)))][column])\n",
    "        print(f'{column}: {outliers} outliers ({outliers/df.shape[0]*100:.2f}%)')\n",
    "        outlier_count+= outliers\n",
    "    return outlier_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_outliers(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate z-scores for only numeric columns without creating dummies\n",
    "outlier_mask = (np.abs(df2.select_dtypes(include=np.number).apply(zscore)) < 3).all(axis=1)\n",
    "\n",
    "# Filter the DataFrame based on the outlier mask and retain the original column structure\n",
    "df3 = df2[outlier_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_outliers(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=df3.copy()\n",
    "for column in df4.select_dtypes(include=np.number).columns:\n",
    "    df4[column]=np.clip(df4[column], df4[column].quantile(0.25)-1.5*(df4[column].quantile(0.75)-df4[column].quantile(0.25)), df4[column].quantile(0.75)+1.5*(df4[column].quantile(0.75)-df4[column].quantile(0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_outliers(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dumies\n",
    "df5 = pd.get_dummies(df4, columns=['real_state','neighborhood'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert boolean to numeric\n",
    "cols = df5.select_dtypes(['bool'])\n",
    "for i in cols.columns:\n",
    "    df5[i] = df5[i].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Min-Max Scaling\n",
    "min_max_scaler = MinMaxScaler()\n",
    "df5mm = pd.DataFrame(min_max_scaler.fit_transform(df5), columns=df5.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5mm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df5mm.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapop = data.merge(unique_combinations_sorted.head(10), on=['rooms', 'bathroom'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_combinations_sorted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Data Preprocesing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preprocessed data on a copy of the original dataset named df2\n",
    "- Created a new category \"unknown\" in the variable 'real_state' replacing NaN\n",
    "- Removed the variable \"Unnamed: 0\" which had no value for modeling\n",
    "- There are outliers in all variables. df2.shape:(8188, 9)\n",
    "- Applied the Z-score method, which removes outliers with more than 3 standard deviations. Some variables with a relevant percentage of outliers still remain. df3.shape:(7742, 9)\n",
    "- Limited outliers to respective whisker boundaries. df4.shape:(7742, 9)\n",
    "- Created dummy variables for variables 'real_state' and 'neighborhood'. df5.shape:(7742, 20)\n",
    "- Boolean variables were converted to numeric\n",
    "- Min-Max scaling was applied. The dataset has features with different scales, normalization ensures that no feature dominates the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA (pre-modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "univariate_numerical(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "corr_matrix = data.select_dtypes(include=np.number).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation matrix as heatmap\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the sorted correlation table\n",
    "corr_unstacked = corr_matrix.unstack() # Unstack the correlation matrix\n",
    "corr_unstacked = corr_unstacked.reset_index() # Reset the index to get 'variable1' and 'variable2' as columns\n",
    "corr_unstacked.columns = ['variable1', 'variable2', 'correlation']# Rename the columns for better understanding\n",
    "corr_unstacked = corr_unstacked[corr_unstacked['variable1'] != corr_unstacked['variable2']] # Remove self-correlations by filtering out rows where variable1 == variable2\n",
    "corr_unstacked = corr_unstacked.drop_duplicates(subset=['correlation']) # Drop duplicates to keep only one entry per variable pair\n",
    "sorted_corr = corr_unstacked.sort_values(by='correlation', key=abs, ascending=False) # Sort the DataFrame by the absolute value of correlation\n",
    "sorted_corr # Display the sorted correlation table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to create the corr_lvl column\n",
    "sorted_corr['corr_lvl'] = sorted_corr['correlation'].apply(categorize_correlation)\n",
    "sorted_corr['corr_lvl'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on EDA (pre-modeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The shape of the data for modeling (7742 , 20) does not account for outliers\n",
    "- The data for modeling have no missing values ​​and all variables are numeric and scaled\n",
    "- Low correlation between variables, with only a couple of variables having a high correlation (bathroom and square footage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying the independent  and dependent variables\n",
    "X = data.drop([\"price\"], axis=1)\n",
    "Y = data[\"price\"]\n",
    "\n",
    "# adding a constant to the independent variables\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# splitting data in train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, random_state=1)\n",
    "\n",
    "# Checking training and test sets.\n",
    "print(\"Shape of Training set : \", x_train.shape)\n",
    "print(\"Shape of test set : \", x_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to evaluate and return the model's metrics\n",
    "def evaluate_model(model, x_test, y_test):\n",
    "    y_pred = model.predict(x_test)\n",
    "    metrics = {\n",
    "        \"MAE\": mean_absolute_error(y_test, y_pred),\n",
    "        \"MSE\": mean_squared_error(y_test, y_pred),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "        \"R2 Score\": r2_score(y_test, y_pred)\n",
    "    }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize an empty DataFrame to store results\n",
    "results_df = pd.DataFrame(columns=[\"Model\", \"MAE\", \"MSE\", \"RMSE\", \"R2 Score\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dictionary of regression models to try\n",
    "regression_models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Lasso Regression\": Lasso(),\n",
    "    \"Ridge Regression\": Ridge(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "    \"Random Forest\": RandomForestRegressor(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsRegressor(),\n",
    "    \"Support Vector Regressor\": SVR()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loop through each model, train it, evaluate it, and store results\n",
    "for model_name, model in regression_models.items():\n",
    "    model.fit(x_train, y_train)\n",
    "    metrics = evaluate_model(model, x_test, y_test)\n",
    "    metrics[\"Model\"] = model_name  # Add model name for reference\n",
    "    results_df = pd.concat([results_df, pd.DataFrame([metrics])], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.007376</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.023725</td>\n",
       "      <td>0.990614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.010450</td>\n",
       "      <td>0.001089</td>\n",
       "      <td>0.033001</td>\n",
       "      <td>0.981840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Support Vector Regressor</td>\n",
       "      <td>0.041981</td>\n",
       "      <td>0.003182</td>\n",
       "      <td>0.056408</td>\n",
       "      <td>0.946942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>0.042237</td>\n",
       "      <td>0.004036</td>\n",
       "      <td>0.063528</td>\n",
       "      <td>0.932704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>0.042269</td>\n",
       "      <td>0.004047</td>\n",
       "      <td>0.063616</td>\n",
       "      <td>0.932517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.059409</td>\n",
       "      <td>0.009173</td>\n",
       "      <td>0.095774</td>\n",
       "      <td>0.847047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>0.196899</td>\n",
       "      <td>0.059972</td>\n",
       "      <td>0.244891</td>\n",
       "      <td>-0.000026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model       MAE       MSE      RMSE  R2 Score\n",
       "4             Random Forest  0.007376  0.000563  0.023725  0.990614\n",
       "3             Decision Tree  0.010450  0.001089  0.033001  0.981840\n",
       "6  Support Vector Regressor  0.041981  0.003182  0.056408  0.946942\n",
       "0         Linear Regression  0.042237  0.004036  0.063528  0.932704\n",
       "2          Ridge Regression  0.042269  0.004047  0.063616  0.932517\n",
       "5       K-Nearest Neighbors  0.059409  0.009173  0.095774  0.847047\n",
       "1          Lasso Regression  0.196899  0.059972  0.244891 -0.000026"
      ]
     },
     "execution_count": 852,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Display the results DataFrame\n",
    "results_df.sort_values(by=\"R2 Score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.007376</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.023725</td>\n",
       "      <td>0.990614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.010450</td>\n",
       "      <td>0.001089</td>\n",
       "      <td>0.033001</td>\n",
       "      <td>0.981840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Support Vector Regressor</td>\n",
       "      <td>0.041981</td>\n",
       "      <td>0.003182</td>\n",
       "      <td>0.056408</td>\n",
       "      <td>0.946942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>0.042237</td>\n",
       "      <td>0.004036</td>\n",
       "      <td>0.063528</td>\n",
       "      <td>0.932704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>0.042269</td>\n",
       "      <td>0.004047</td>\n",
       "      <td>0.063616</td>\n",
       "      <td>0.932517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.059409</td>\n",
       "      <td>0.009173</td>\n",
       "      <td>0.095774</td>\n",
       "      <td>0.847047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>0.196899</td>\n",
       "      <td>0.059972</td>\n",
       "      <td>0.244891</td>\n",
       "      <td>-0.000026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model       MAE       MSE      RMSE  R2 Score\n",
       "4             Random Forest  0.007376  0.000563  0.023725  0.990614\n",
       "3             Decision Tree  0.010450  0.001089  0.033001  0.981840\n",
       "6  Support Vector Regressor  0.041981  0.003182  0.056408  0.946942\n",
       "0         Linear Regression  0.042237  0.004036  0.063528  0.932704\n",
       "2          Ridge Regression  0.042269  0.004047  0.063616  0.932517\n",
       "5       K-Nearest Neighbors  0.059409  0.009173  0.095774  0.847047\n",
       "1          Lasso Regression  0.196899  0.059972  0.244891 -0.000026"
      ]
     },
     "execution_count": 853,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.sort_values(by=\"MAE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.007376</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.023725</td>\n",
       "      <td>0.990614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.010450</td>\n",
       "      <td>0.001089</td>\n",
       "      <td>0.033001</td>\n",
       "      <td>0.981840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Support Vector Regressor</td>\n",
       "      <td>0.041981</td>\n",
       "      <td>0.003182</td>\n",
       "      <td>0.056408</td>\n",
       "      <td>0.946942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>0.042237</td>\n",
       "      <td>0.004036</td>\n",
       "      <td>0.063528</td>\n",
       "      <td>0.932704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>0.042269</td>\n",
       "      <td>0.004047</td>\n",
       "      <td>0.063616</td>\n",
       "      <td>0.932517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.059409</td>\n",
       "      <td>0.009173</td>\n",
       "      <td>0.095774</td>\n",
       "      <td>0.847047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>0.196899</td>\n",
       "      <td>0.059972</td>\n",
       "      <td>0.244891</td>\n",
       "      <td>-0.000026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model       MAE       MSE      RMSE  R2 Score\n",
       "4             Random Forest  0.007376  0.000563  0.023725  0.990614\n",
       "3             Decision Tree  0.010450  0.001089  0.033001  0.981840\n",
       "6  Support Vector Regressor  0.041981  0.003182  0.056408  0.946942\n",
       "0         Linear Regression  0.042237  0.004036  0.063528  0.932704\n",
       "2          Ridge Regression  0.042269  0.004047  0.063616  0.932517\n",
       "5       K-Nearest Neighbors  0.059409  0.009173  0.095774  0.847047\n",
       "1          Lasso Regression  0.196899  0.059972  0.244891 -0.000026"
      ]
     },
     "execution_count": 854,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.sort_values(by=\"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.007376</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.023725</td>\n",
       "      <td>0.990614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.010450</td>\n",
       "      <td>0.001089</td>\n",
       "      <td>0.033001</td>\n",
       "      <td>0.981840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Support Vector Regressor</td>\n",
       "      <td>0.041981</td>\n",
       "      <td>0.003182</td>\n",
       "      <td>0.056408</td>\n",
       "      <td>0.946942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>0.042237</td>\n",
       "      <td>0.004036</td>\n",
       "      <td>0.063528</td>\n",
       "      <td>0.932704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>0.042269</td>\n",
       "      <td>0.004047</td>\n",
       "      <td>0.063616</td>\n",
       "      <td>0.932517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.059409</td>\n",
       "      <td>0.009173</td>\n",
       "      <td>0.095774</td>\n",
       "      <td>0.847047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>0.196899</td>\n",
       "      <td>0.059972</td>\n",
       "      <td>0.244891</td>\n",
       "      <td>-0.000026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model       MAE       MSE      RMSE  R2 Score\n",
       "4             Random Forest  0.007376  0.000563  0.023725  0.990614\n",
       "3             Decision Tree  0.010450  0.001089  0.033001  0.981840\n",
       "6  Support Vector Regressor  0.041981  0.003182  0.056408  0.946942\n",
       "0         Linear Regression  0.042237  0.004036  0.063528  0.932704\n",
       "2          Ridge Regression  0.042269  0.004047  0.063616  0.932517\n",
       "5       K-Nearest Neighbors  0.059409  0.009173  0.095774  0.847047\n",
       "1          Lasso Regression  0.196899  0.059972  0.244891 -0.000026"
      ]
     },
     "execution_count": 855,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.sort_values(by=\"RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Model Building\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order to make statistical inferences from a logistic regression model, it is important to ensure that there is no multicollinearity present in the data.\n",
    "- Data split 70/30. Shape of Training set :  (5419, 20), Shape of test set :  (2323, 20)\n",
    "- Based on the provided results, here are some conclusions you can draw about the performance of each model:\n",
    "- Performance Metrics:\n",
    "    - **MAE (Mean Absolute Error)**: Measures the average magnitude of errors in a set of predictions, without considering their direction.\n",
    "    - **MSE (Mean Squared Error)**: Measures the average of the squares of the errors, giving more weight to larger errors.\n",
    "    - **RMSE (Root Mean Squared Error)**: The square root of MSE, providing error in the same units as the target variable.\n",
    "    - **R2 Score (Coefficient of Determination)**: Indicates how well the model's predictions approximate the real data points. A value closer to 1 indicates a better fit.\n",
    "- **Random Forest**: **Best Performance**. It has the lowest MAE (0.007370), MSE (0.000556), and RMSE (0.023589), and the highest R2 Score (0.990722), indicating it is the most accurate model among the ones tested.\n",
    "- **Decision Tree**: **Second Best**. It also performs very well with low MAE (0.010382), MSE (0.001028), and RMSE (0.032058), and a high R2 Score (0.982863).\n",
    "- **Support Vector Regressor (SVR)**: **Good Performance**. It has a relatively low MAE (0.041981), MSE (0.003182), and RMSE (0.056408), with a high R2 Score (0.946942).\n",
    "- **Linear Regression and Ridge Regression**: Similar Performance. Both have similar metrics with MAE around 0.042, MSE around 0.004, RMSE around 0.063, and R2 Score around 0.93, indicating decent performance.\n",
    "- **K-Nearest Neighbors (KNN)**: Moderate Performance. It has higher MAE (0.059409), MSE (0.009173), and RMSE (0.095774), with a lower R2 Score (0.847047), indicating it is less accurate compared to the top models.\n",
    "- **Lasso Regression**: Poor Performance: It has the highest MAE (0.196899), MSE (0.059972), and RMSE (0.244891), with a negative R2 Score (-0.000026), indicating it performs poorly on this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
